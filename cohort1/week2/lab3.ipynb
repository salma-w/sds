{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Lab 3\n",
    "\n",
    "The illusion of conversation, and multi-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from litellm import completion\n",
    "from IPython.display import Markdown, display\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic.*\")\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"My name is Ryan.\"}\n",
    "]\n",
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi there!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I help you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"My name is Ryan.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Ryan! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "]\n",
    "\n",
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "The reason I show you this is to emphasize that every call to the LLM is completely stateless.\n",
    "\n",
    "The sense that we are engaged in a chat with the LLM that takes place over several minutes is an illusion, enabled by the trick of passing in the entire conversation so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the cost - this is the cost of the entire conversation\n",
    "# Any guesses on how much this is?\n",
    "\n",
    "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time \"training\" with multi-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions in English and then in Spanish.\n",
    "\"\"\"\n",
    "user_prompt = \"What's the meaning of life?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "display(Markdown(response.choices[0].message.content))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that answers questions in English and then in Spanish.\n",
    "\n",
    "For example, if the user asks 'What is 2+2' then you would answer as follows:\n",
    "\n",
    "## My reply in English is:\n",
    "Four\n",
    "\n",
    "## My reply in Spanish is:\n",
    "Cuatro\n",
    "\n",
    "\"\"\"\n",
    "user_prompt = \"What's the meaning of life?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "response = completion(model=\"gpt-4.1-mini\", messages=messages)\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is this \"training\"?\n",
    "\n",
    "It appears that the model _learned_ the format that we wanted, based on the example given.\n",
    "\n",
    "But there was no training going on in the Data Science sense; the trillions of parameters in gpt-4.1-mini weren't changed.\n",
    "\n",
    "It's simply that when predicting the next tokens to come after our question, the model is more likely to predict tokens that would follow the same pattern as the examples in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
